一、顶层设计与核心定位简述

1.1 项目愿景

AgencyBench定位：Agent生产力指数（Agency Productivity Index）
- 对标APEX等顶尖评测基准
- 具有清晰的哲学故事线和商业化潜力
- 评测端到端Agent工作流解决复杂任务的能力
  
1.2 核心差异化

1. 面向Agent，非基础模型：评测完整工作流（如Deep Research、Claude Code）而非单纯语言能力
2. "In-Context"评测哲学：为每个任务提供Ground Truth或参考答案，实现精准、可解释的评测
3. 职业导向场景：以真实职业（金融分析师、政策研究员、科研人员等）为场景主线，符合AI替代人类工作的宏观叙事
  
1.3 统一评估框架

- 所有Query通过统一的Benchmark Anything Agent和In-Context Agent-as-a-Judge评估
- 不因行业场景而异，确保评测的一致性和可比性
  

---

二、核心构建哲学（多维融合）

2.1 以终为始的逆向工程（根本方法论）

黄金法则：先有标准答案（Ground Truth），再有问题（Query）

错误思路（传统）：
有任务想法 → 思考AI如何回答 → 思考如何评估

正确思路（AgencyBench）：
找到高质量现实成果(Ground Truth) → 逆向分析需要解决什么问题 → 设计Query → 验证可执行性

2.2 教师命题范本（设计视角）

核心要求：每个Query的设计过程应模拟一位专业领域教师为学生设计课程作业/考试题目

- 目标明确：清晰的考察目标（要考察什么能力？）
- 答案可判：有标准答案或评分准则（Rubrics）
- 难度分级：基础题、综合题、创新题对应L3-L5
  
2.3 业务价值导向（价值锚点）

根本问题：一个真实的人类专家（公务员、金融分析师、医生）需要完成的、最具价值的深度工作是什么？

价值度量：AI价值 = 人力成本 × 人力耗时

任务类型聚焦：
- ✅ **要做**：使用AI生成报告、方案、分析（"使用平台"）
- ❌ **不做**：要求AI开发系统、平台、架构（"开发平台"）
  
2.4 三E原则（质量基准）

1. Examining（考察能力）：有效触发并评估高阶能动性
2. Executable（易于作答）：任务清晰，上下文充分
3. Evaluable（便于评估）：产出有相对客观的评估标准
  

---

三、L3-L5任务定义与设计范式（精炼版）

级别
核心比喻与耗时
设计范式与核心动作
评估的关键
必须满足的"绿色"标准
必须避免的"红色"陷阱
L3：基础题  （数小时-1天）
课后习题/单元实验  考察单一知识点或技能的掌握和调试能力
实现一个封闭的、有明确验收标准的模块  • 核心动作：实现、调试、验证  • 范例："给定注意力机制代码框架，实现GQA模块并通过单元测试"
• 结果导向：交付模块功能是否正确？  • 是否通过测试？  • 代码质量如何？
1. 使用Claude Code等单步工具在几小时内可完成七八成  2. 目标明确、封闭，有清晰完成标准  3. 易于验证，产出有唯一或有限优解
1. 【禁】需要多步复杂推理或规划
2. 【禁】需要训练模型或大量计算资源
3. 【禁】目标模糊开放（如"预测市场趋势"）
L4：综合题  （数天-1周）
课程大作业/论文复现  考察信息整合、系统实现、复现验证能力
复现一个已有的、复杂的成果  • 核心动作：调研、规划、实现、对比  • 范例："复现VPTQ量化算法论文核心实验，对比结果与论文性能指标"
• 结果对比：复现成果与Ground Truth的吻合度  • 过程评估：方案是否合理？调研是否充分？
1. 基于已存在的高质量公开成果（论文、研报）  2. 需要Deep Research进行信息搜索、整合和分析  3. 最终产出可与Ground Truth客观对比
1. 【禁】Ground Truth不存在或无法获取
2. 【禁】需要从头训练大模型或进行长时间模型训练（如需要8块H100）
3. 【禁】成果好坏没有相对客观评判标准
L5：开放题/创新题  （1个月以上）
毕业设计/战略研究  考察在约束条件下进行创新性思考和战略规划
解决开放的、无现成答案的战略性问题  • 核心动作：Deep Research与Claude Code交替循环（调研→方案→验证→再调研）  • 范例："仅基于2025年1月前信息，设计应对AI算力挑战的芯片架构创新方案"
• 方案合理性：与事后验证或领域公认挑战的契合度  • 创新性与逻辑：创新点、逻辑严密性和可行性  • 不追求唯一标准答案
1. 解决开放性、战略性问题，答案不唯一  2. 考验创新思维和战略规划能力  3. 方案合理性可与未来真实事件或领域挑战进行回溯对比
1. 【禁】任务本质上不可能完成（如"用1万元造出H100"）
2. 【禁】评估标准完全主观，无法衡量
3. 【禁】设计成无法验证的"空想"任务

工具使用界定

- L4任务：通常仅依赖一种核心Agent模式（仅Claude Code或仅Deep Research）
- L5任务：必须要求Deep Research和Claude Code等多种能力交替、循环使用
  

---

四、逆向构建法详细工作流（按当前实际操作口径修订）

4.1 正向任务（positive）

步骤一：规格与场景冻结（输入准备）
- 明确行业/职业与角色：给出真实委托方与“为什么做”。
- 场景与口径：限定业务范围、时间窗口（如“仅用某日期前资料”）、信息源限制（如“仅公开渠道”）。
- 级别与取向：L3/L4/L5；正向为交付闭环。
- 关注点：交付要求（格式、长度、质量门槛）、评分要点（可客观判分的抓手）、搜索意图（关键词/主题）。

步骤二：公开检索与基准锁定（Ground Truth 先行）
- 候选收集：通过通用检索服务收集公开资料，优先权威与可下载载体（报告/论文PDF、官方页面、代码库、模型卡）。
- 质量与可达：剔除跳转站/图片/脚本等非文本资源；确保链接可打开、内容完整、与场景密切相关。
- 主参考确定：锁定1份“主参考资料”（优先PDF或官方长文/权威页），不足时以高权威网页替代；必要时补充佐证来源。
- 辅助参考：再选2–3条“辅助参考”，避免与主参考同域、避免冗余，起到背景与拓展作用。

步骤三：任务结构化（四要素齐备）
- 角色与背景：像领导/导师/客户的真实委托，交代目标、期限、验收口径。
- 任务目标：条目化、可执行；避免互相掣肘目标；对应层级粒度。
- 输入与资源：列出“提供的资料/允许检索范围/时间窗口/引用规范”。
- 交付要求：明确格式、长度、质量门槛（阈值/一致性/覆盖度等），以及单一主交付；
- 评分要点：给出可客观复核的Rubric（核查清单/对比点/阈值）。
- 参考清单与评测基准：对外提供去重后的参考清单；评测基准（主/辅参考）仅内部留档。

步骤四：SOP对齐与净化（护栏落实）
- 训练红线（L4）：将“训练/大算力/长时实验”等表述统一规约为“验证/推理/复现”，并加入时间与资源护栏（如≤若干工时/设备限制）。
- 用语与泄露：对外统一称“参考资料/提供的资料”，禁止出现“Ground Truth”；
- 清单去泄露：面向执行者的“提供的资料”不得包含主参考链接或与之同主域的直链；必要时以其他参考回填。

步骤五：打包与出库（两视图+最小集）
- 评审视图：完整任务说明、参考清单、基准资料元信息；必要PDF优先入包（保守分发）。
- 求解视图（可选）：隐藏评测字段（基准/标准答案），仅保留对外可见信息，防止泄露评测基准。
- 精简包：仅保留任务说明（便于人读）、data room（参考清单/关键PDF）与基准目录（元信息）；目录结构统一，便于流转与审计。
- 溯源：记录生成批次标签、来源概述与时间窗口，支持复核与回滚。

步骤六：失败与回退策略（标注与清退）
- 检索失败/访问不稳：允许先出结构化任务与链接清单（不强制下载），对条目标注“占位”，在后续批次补齐或下线。
- 生成失败：可用模板化结果补位，但需在备注中显式标注并安排人工复核；模板或占位条目设定清退SLA（建议≤2周）。
- PDF不稳定：提供替代路径或等价来源；无法下载时仅保留链接并在备注说明。

步骤七：复核与验收（自动+人工）
- 快速核对（≤3分钟/条）：读场景与目标→判层级与取向→看交付/评分抓手→点主参考可达与权威性→扫参考清单与安全用语。
- 抽检与回归：每批抽检≥20%；对占位/模板条目持续复检直至转正或下线；对关键行业条目做深度复现抽查。
- 验收结论：通过/警告（修改后通过）/不通过（退回重做），并记录退修原因。

注意：
- 不允许“多头交付”与互斥要求；
- 引用必须真实可溯源，禁止捏造；
- 所有条目须避免隐私/内部/敏感政治内容，必要时做中性化处理（“某市/某公司”）。

4.2 负向任务（inverse）

目标：构造看似合理但本质不可完成或前提有误的任务，用于评测批判性思维与“拒绝执行”的专业性。

构造要点：
- 设计矛盾：违反已知规律/关键约束，或引入不可复现/错误数据作为前提；
- 资源边界：设置远超现实资源/能力的约束以触发证伪；
- 证伪路径：给出可复现的验证流程与拒绝阈值（数据/步骤/判据/日志化证据）。

护栏：
- Training-free：禁止从头训练或昂贵/长时算力；
- 合规来源：仅使用公开、中立、国际化数据；必要时脱敏并设置时间窗口与信息源限制；
- 用语安全：对外统一称“参考资料/提供的资料”，不出现“Ground Truth”。

评估抓手：
- 识别与定位矛盾的准确性；
- 证据链完整性与可重现性（检索/实验/推理日志）；
- 拒绝执行说明的专业性与风险提示；
- 是否避免“继续推进原需求”的惯性。

自检清单：
- 是否能稳定导向“不可行/前提有误”的结论？
- 证伪流程第三方可复现吗（含数据、步骤、判据与日志）？
- 是否显式给出拒绝阈值与退出条件？
五、Query结构模板（强化版）

每个Query必须包含以下四个部分：

5.1 角色与背景 (Role & Context)

目的：设定场景，赋予AI角色感，提供任务背景

要求：
- 描述真实、有代入感，说明"为什么"要做这件事
- 角色与背景真实：设定真实职业角色和业务背景
- 任务源于实践：像该领域专家日常真正遇到的问题，避免"AI味"
  
正例：
你是某市发改委的一名政策分析员。为制定明年的经济发展规划，主任需要一份关于《我市人工智能产业链现状、瓶颈及发展建议》的深度研究报告，要求你在2个工作日内完成初稿。

反例：
请分析以下数据。（❌ 缺乏场景感）

5.2 任务目标 (Task Objectives)

目的：清晰、具体地列出需要完成的事项

要求：
- 使用条目化描述，明确、无歧义
- 目标可交付、可检查
- 基于公开可获取的信息或提供的数据集
  
范例：
请基于提供的资料及公开信息，完成以下工作：
1. 产业链梳理：绘制我市人工智能产业链图谱，至少包含基础层、技术层、应用层三个环节，并列出每个环节的本地代表企业（不少于5家）。
2. 瓶颈分析：从技术、人才、政策、资金四个维度，分析我市AI产业发展的主要瓶颈，每个维度至少指出2个具体问题。
3. 提出建议：针对上述瓶颈，提出3-5条具体、可操作的发展建议。

5.3 输入信息 (Input/Resources)

目的：提供完成任务所必需的信息或数据

要求：
- 可以是文件链接、数据表格、网址列表
- 或指示AI去特定来源搜索
- 确保信息公开可获取
  
范例：
提供的资料：
- [链接] 《某市2024年政府工作报告》
- [链接] 《中国人工智能产业发展白皮书2024》
  
- 需要你自行搜索的信息：
- 请通过搜索引擎获取"某市 人工智能 企业名录"、"某市 人才引进政策"等最新公开信息以补充分析。

5.4 交付物要求 (Deliverable Requirements)

目的：明确规定最终产出的形式、格式和内容要求

要求：
- 具体、可衡量
- 让AI和评估者都清楚"什么样的产出是好的"
- 可参考过往报告格式，但不应过度限定，留给AI发挥空间
  
范例：
请交付一份完整的分析报告：
- 格式：Markdown格式
- 内容：报告需结构完整，包含摘要、现状分析、瓶颈问题、发展建议等部分。对瓶颈的分析需有数据或案例支撑。
- 长度：正文不少于1500字


---

六、Query质量核心维度与全面检查清单

6.1 核心维度

维度1：真实性与场景化
[] 角色与背景真实：是否设定了真实的职业角色和业务背景？
[] 任务源于实践：是否像该领域专家日常真正遇到并需要解决的问题？
[] 专业度对标：对比APEX等标杆，专业度和复杂度是否接近？
[] 避免AI味：隐藏AI背景后，领域专家会认为这是值得人类完成的任务吗？

维度2：可验证性
[] Ground Truth明确：是否已为该Query确定了高质量、可获取的Ground Truth？
[] 能够大规模验证：是否是training-free的任务，还是需要算力进行长时间模型训练？
[] 评估标准清晰：教师能否根据Ground Truth或Rubrics进行相对客观评分？
[] 答案可判：有标准答案或明确的评分准则吗？

维度3：层级准确性
[] 工具使用界定清晰：
- L4：仅依赖一种核心Agent模式
- L5：必须Deep Research和Claude Code交替循环使用
[] 耗时符合预期：任务预期完成时间是否与定义匹配？
- L3：小时/天
- L4：天/周
- L5：周/月
[] 红线检查：是否跳出了对应级别的"红色陷阱"？

维度4：题型多样性
[] 避免重复题型：新Query与已有Query在任务类型上是否有差异性？
[] 工作流多样性：同一场景下，是否包含不同类型的工作流程？
- 例如：科研场景不应只有"复现论文"，还可有"制作宣讲PPT"、"撰写领域综述"等
  
6.2 快速自检清单（提交前必查）

真实性与专业性
[] 读起来像真实的业务需求（领导布置的任务、导师布置的课题）吗？
[] 一个领域专家会认为这是值得花时间完成的专业任务吗？
[] 对比APEX等标杆，我的Query在专业度和复杂度上是否接近？

可执行性（可行性）
[] 是否已用目标Agent平台（Claude Code/Deep Research）实际运行过？
[] 任务所需的所有输入信息是否明确提供或可公开、轻松获取？
[] 是否避免了对特殊、昂贵或不可能获得的基础设施算力的依赖？

可验证性
[] 是否有清晰、可获取的Ground Truth作为评判基准？
[] 如果没有标准答案，是否有基于参考答案的评估标准（Rubrics）？

层级准确性
[] 根据"红线标准"，是否高估或低估了任务级别？

逆向逻辑验证
[] Ground Truth是否先行？在撰写Query前是否已明确确定并验证？
[] 这个Query是否从Ground Truth反向推导出来？而非凭空想象？
[] 教师视角：这个Query配上Ground Truth，能否作为合格的试卷题目和标准答案？

框架通用性
[] 这个Query是否不依赖特定、神秘的行业知识？
[] 能否被统一的Benchmark Anything Agent理解并生成评估标准？


---

七、场景与数据选择指南

7.1 优先选择（安全与可开源）

数据源类型：公开、中立、国际化的数据源

- 金融：上市公司年报、公开行业研报、黄金/外汇价格
- 政务：各城市统计局公开年度报告、公共数据集（交通流量）
- 科研：arXiv学术论文、公开专利数据集
- 医疗：PubMed文献、WHO发布的公共健康报告
  
7.2 必须避免

- ❌ 涉及个人隐私、内部统计、敏感政治话题的数据和任务
- ❌ 中国特色的政治术语、敏感人名、地名
- ❌ 内部数据引用
  
7.3 脱敏处理

- 移除敏感信息，使用"某市"、"某公司"、"根据公开数据"等中性表述
- 确保符合国际开源社区标准
  
7.4 安全与可控性

时间窗口限制：要求AI只能参考某个截止日期之前的信息
- 例如："仅使用2024年以前的文献"
  
信息源限制：提供特定的、经过筛选的数据集或文档库，而非开放互联网搜索


---

八、完整构建流程与验证闭环

步骤零：学习范例（关键起点）
- 行动：仔细研究刘锦绣提到的约10个正面范例
- 目的：理解"好Query"的具体形态，建立质量基准
  
步骤一：构思与撰写
1. 锚定业务场景与价值点
2. 使用场景探索Prompt与AI进行头脑风暴
3. 逆向验证"答案"的存在性与可获取性
4. 定义任务级别（L3-L5）
5. 严格套用四部分结构模板
  
步骤二：自我验证与修正（强制性）

立即运行：完成Query初稿后，立即在可用Agent平台上运行

观察并记录：
- Agent是否理解了核心任务？
- 执行过程是否流畅？在哪一步卡住？
- 最终产出是否接近预期？
  
修正Query：根据运行结果反哺修改Query，解决不明确、不可行的地方（可能需迭代多次）

步骤三：质量自检
- 逐条对照"六、Query质量核心维度与全面检查清单"
- 填写检查记录
  
步骤四：Peer Review
- 将自检清单和测试结果一并提交给同伴评审
- 评审重点：是否跳出"红色陷阱"？是否达到"绿色标准"？
  
步骤五：生成评测报告

【Query人工评测报告】
Query ID/名称：[填写]
测试模型/平台：Deep Research (奇绩 OpenRouter)
测试结果概要：AI是否理解了任务核心？执行流程是否合理？
主要问题：是否出现因Query表述不清导致的歧义或错误？
优化建议：对Query的修改建议。
结论：[ ] 通过，可入库  [ ] 需修改后重测

步骤六：最终优化与入库
- 根据测试反馈进行最后修订
- 正式入库
  
步骤七：客户验证（终极标准）
- 时机：形成一批相对成熟的Query后
- 方式：作为Demo案例，直接用于和相关领域客户（如卫健委、金融机构）沟通
- 目标：听取客户反馈，验证Query的业务真实性和价值认可度
  

---

九、质量保障与协作流程

9.1 分层级构建与评审

L3/L4任务：作为当前构建和评测的重点
- 每个任务必须严格遵循逆向构建法
- 经过真实Agent平台测试和人工评审
  
L5任务：作为思路展示和未来方向
- 在论文中呈现设计思路和挑战
- 暂不投入大量资源进行自动化评测体系构建
- 现阶段策略：展示思路，暂不深度评测
  
9.2 团队协作与分工

场景分配：将不同职业场景（金融、法律、医疗、HR等）分配给团队成员

交叉评审：定期召开评审会，团队成员相互评审Query，重点检查：
- 是否符合层级定义？（特别是L3/L4的区分）
- Ground Truth是否高质量、易获取？
- Query是否真实、清晰、可执行？
  
9.3 引入专家反馈

时机：在形成一批相对成熟的Query后

方式：邀请奇绩内部行业专家或相关领域校友，以**评审者（Reviewer）**而非出题者（Setter）的身份，对Query的业务真实性和价值进行反馈

优势：比直接请他们出题成本更低、可行性更高

9.4 内部评审会（Mandatory）

评审标准：
1. 真实性：这个Query看起来是否像一份真实的工作任务？
2. 可执行性：所需数据是否公开可得？任务描述是否清晰无歧义？
3. 价值度：这个任务如果由人完成，是否需要数小时甚至数天？能否清晰估算其人力成本？
  

---

十、附录：任务范例
10.1 L3任务范例
暂时无法在飞书文档外展示此内容

10.2 L4任务范例
暂时无法在飞书文档外展示此内容

10.3 L5任务范例

正例（战略规划）：
角色与背景：假设现在是2025年1月。你是某大型科技公司的战略顾问。

任务目标：仅基于2025年1月之前的信息，为该公司设计一个2025年度的组织架构与人才战略创新方案，以应对即将到来的AI技术变革。

具体要求：
1. 分析当前AI技术发展趋势和行业挑战
2. 识别公司现有组织架构的潜在瓶颈
3. 提出创新性的组织架构调整方案
4. 设计配套的人才招聘和培养策略
5. 评估方案的可行性和潜在风险
  
5. 评估方式：我们将对比你提出的方案与该公司在2025年7月实际公布的方案在核心思路上的一致性。


---

结语

本SOP（V8.0）整合了7次迭代的所有核心洞察，将Query构建从"技巧"提升为**系统性方法论**。它不仅是构建指南，更是关于"如何构建有影响力的AI Agent生产力标准"的完整思想体系。

请团队成员：
1. 深刻理解"以终为始"的逆向工程哲学
2. 严格执行Ground Truth先行原则
3. 优先保障L3/L4任务的质量与规模
4. 在构思每个Query时反复对照检查清单进行自我质询
  
核心铭记：我们不是在出题，而是在**定义AI Agent的生产力标准**。
